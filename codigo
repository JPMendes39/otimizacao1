import numpy as np
from math import *



def golden_section_searcher(X, d, prev_val, lower, upper, epsilon):

    phi = 1.618
    x1 = upper - ((phi - 1)*(upper - lower))
    x2 = lower + ((phi - 1)*(upper - lower))
    val = x1
    
    param2 = X - np.dot(x2, d)
    param2 = param2.tolist()
    
    param1 = X - np.dot(x1, d)
    param1 = param1.tolist()
    
    if f(param2) < f(param1):
        if x1 > x2:
            upper = x1
        else:
            lower = x1

    else:
        if x2 > x1:
            upper = x2
        else:
            lower = x2

    if abs(prev_val - val) <= epsilon:
        return val
    else:
        return golden_section_searcher(X, d, val, lower, upper, epsilon)


def gradiente(f, X):
    h = 0.0000001
    grad = []
    
    for i in range(len(X)):
        E = np.zeros(len(X))
        E[i] = h
        vals = X + E
        grad.append((f(vals) - f(X))/h)
            
    return grad


def difference(X, Y):
    total = 0
    
    for i in range(len(X)):
        total = total + abs(X[i] - Y[i])
    total = total / len(X)
    
    return total


def steepest_descent(X, epsilon):
    iteracoes = 0
    
    while True:
        d = gradiente(f, X)
        x_prev = X
        learning_rate = golden_section_searcher(X, d, 1, -10, 10, 0.0001)
        X = X - np.dot(learning_rate, d)
        X = X.tolist()

        iteracoes += 1
        
        if difference(x_prev, X) < epsilon:
            return x_prev, f(x_prev), iteracoes
        
        
    return x_prev, f(x_prev), iteracoes


def f(x):
    return 4*(x[0]**2) - 4*x[0]*x[1] + 2*(x[1]**2)
    
print(steepest_descent([1,1], 0.0001))
